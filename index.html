<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Top 10 Open Source LLMs (Late 2024/2025) | ainfo.dev</title>
    <meta
      name="description"
      content="Discover the best open source Large Language Models (LLMs) of late 2024 and 2025. Comprehensive list including Llama 3.1, DeepSeek V3, Mistral, and more."
    />
    <meta
      name="keywords"
      content="Open Source LLM, AI Models, Llama 3.1, DeepSeek, Mistral, Qwen, Artificial Intelligence, Best LLMs 2025"
    />
    <meta
      property="og:title"
      content="Top 10 Open Source LLMs (Late 2024/2025)"
    />
    <meta
      property="og:description"
      content="The definitive guide to the best open source AI models available right now."
    />
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://ainfo.dev" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap"
      rel="stylesheet"
    />
    <link rel="stylesheet" href="style.css" />
  </head>
  <body>
    <div class="background-gradient"></div>

    <header>
      <nav class="container">
        <div class="logo">ainfo.dev</div>
        <!-- Future nav links can go here -->
      </nav>
      <div class="hero container">
        <h1>Top 10 Open Source LLMs</h1>
        <p class="subtitle">
          The most powerful, efficient, and capable open-weight models defining
          the AI landscape in late 2024 and 2025.
        </p>
      </div>
    </header>

    <main class="container">
      <div class="llm-grid">
        <!-- 1. Llama 3.1 -->
        <a href="llama-3-1.html" class="llm-card-link">
          <article class="llm-card">
            <div class="card-header">
              <span class="rank">#1</span>
              <h2>Llama 3.1</h2>
              <span class="badge meta">Meta AI</span>
            </div>
            <p class="description">
              The reigning champion of open weights. With 8B, 70B, and the massive
              405B variants, Llama 3.1 brings GPT-4 class performance to the open
              community. Excellent for general-purpose tasks, reasoning, and
              coding.
            </p>
            <div class="tags">
              <span>General Purpose</span>
              <span>405B Parameters</span>
              <span>Industry Standard</span>
            </div>
          </article>
        </a>

        <!-- 2. DeepSeek V3/R1 -->
        <a href="deepseek-v3.html" class="llm-card-link">
            <article class="llm-card">
                <div class="card-header">
                    <span class="rank">#2</span>
                    <h2>DeepSeek V3 & R1</h2>
                    <span class="badge deepseek">DeepSeek AI</span>
                </div>
                <p class="description">A powerhouse in reasoning and coding. DeepSeek R1 (Jan 2025) sets new benchmarks in complex math and logic, while V3 offers incredible efficiency. A favorite for developers and researchers.</p>
                <div class="tags">
                    <span>Reasoning</span>
                    <span>Coding</span>
                    <span>Math</span>
                </div>
            </article>
        </a>

        <!-- 3. Mistral Large 2 -->
        <a href="mistral-large-2.html" class="llm-card-link">
            <article class="llm-card">
                <div class="card-header">
                    <span class="rank">#3</span>
                    <h2>Mistral Large 2</h2>
                    <span class="badge mistral">Mistral AI</span>
                </div>
                <p class="description">The European giant. Mistral Large 2 (123B) is a coding and multilingual beast, supporting 80+ languages with a massive context window. Perfect for enterprise-grade applications.</p>
                <div class="tags">
                    <span>Multilingual</span>
                    <span>123B Parameters</span>
                    <span>Long Context</span>
                </div>
            </article>
        </a>

        <!-- 4. Qwen 2.5 -->
        <a href="qwen-2-5.html" class="llm-card-link">
            <article class="llm-card">
                <div class="card-header">
                    <span class="rank">#4</span>
                    <h2>Qwen 2.5</h2>
                    <span class="badge qwen">Alibaba Cloud</span>
                </div>
                <p class="description">The multilingual master. Qwen 2.5 excels in instruction following and logical reasoning across diverse languages. Its performance rivals top-tier closed models in many benchmarks.</p>
                <div class="tags">
                    <span>Multilingual</span>
                    <span>Instruction Following</span>
                    <span>Versatile</span>
                </div>
            </article>
        </a>

        <!-- 5. GPT-OSS -->
        <a href="gpt-oss.html" class="llm-card-link">
            <article class="llm-card">
                <div class="card-header">
                    <span class="rank">#5</span>
                    <h2>GPT-OSS</h2>
                    <span class="badge openai">OpenAI</span>
                </div>
                <p class="description">OpenAI's entry into open weights. The 120B parameter model is optimized for advanced reasoning and agentic workflows, bridging the gap between closed and open ecosystems.</p>
                <div class="tags">
                    <span>Agentic</span>
                    <span>Reasoning</span>
                    <span>Tool Use</span>
                </div>
            </article>
        </a>

        <!-- 6. Gemma 2 -->
        <a href="gemma-2.html" class="llm-card-link">
            <article class="llm-card">
                <div class="card-header">
                    <span class="rank">#6</span>
                    <h2>Gemma 2</h2>
                    <span class="badge google">Google</span>
                </div>
                <p class="description">Built on Gemini research. Gemma 2 offers state-of-the-art performance in lighter weight classes (9B, 27B), making it ideal for efficient deployment without sacrificing reasoning quality.</p>
                <div class="tags">
                    <span>Efficient</span>
                    <span>Gemini-based</span>
                    <span>Lightweight</span>
                </div>
            </article>
        </a>

        <!-- 7. Phi-4 -->
        <a href="phi-4.html" class="llm-card-link">
            <article class="llm-card">
                <div class="card-header">
                    <span class="rank">#7</span>
                    <h2>Phi-4</h2>
                    <span class="badge microsoft">Microsoft</span>
                </div>
                <p class="description">Small but mighty. Phi-4 punches way above its weight class, delivering exceptional reasoning and coding capabilities that run efficiently on consumer hardware.</p>
                <div class="tags">
                    <span>Small Language Model</span>
                    <span>On-Device</span>
                    <span>Coding</span>
                </div>
            </article>
        </a>

        <!-- 8. Grok-1 -->
        <a href="grok-1.html" class="llm-card-link">
            <article class="llm-card">
                <div class="card-header">
                    <span class="rank">#8</span>
                    <h2>Grok-1</h2>
                    <span class="badge xai">xAI</span>
                </div>
                <p class="description">The massive MoE. With 314B parameters, Grok-1 is a beast of a model known for its "spicy" personality and strong general knowledge. A unique option for those with the compute.</p>
                <div class="tags">
                    <span>314B Parameters</span>
                    <span>MoE</span>
                    <span>Creative</span>
                </div>
            </article>
        </a>

        <!-- 9. Yi-1.5 -->
        <a href="yi-1-5.html" class="llm-card-link">
            <article class="llm-card">
                <div class="card-header">
                    <span class="rank">#9</span>
                    <h2>Yi-1.5</h2>
                    <span class="badge yi">01.AI</span>
                </div>
                <p class="description">A strong bilingual contender. Yi-1.5 (34B) offers a great balance of performance and size, excelling in both English and Chinese tasks with improved coding skills.</p>
                <div class="tags">
                    <span>Bilingual</span>
                    <span>34B Parameters</span>
                    <span>Balanced</span>
                </div>
            </article>
        </a>

        <!-- 10. Command R+ -->
        <a href="command-r-plus.html" class="llm-card-link">
            <article class="llm-card">
                <div class="card-header">
                    <span class="rank">#10</span>
                    <h2>Command R+</h2>
                    <span class="badge cohere">Cohere</span>
                </div>
                <p class="description">The RAG specialist. Optimized for retrieval-augmented generation and tool use, Command R+ is the go-to open model for building complex, data-driven enterprise assistants.</p>
                <div class="tags">
                    <span>RAG</span>
                    <span>Enterprise</span>
                    <span>Tool Use</span>
                </div>
            </article>
        </a>
      </div>
    </main>

    <footer>
      <div class="container">
        <p>
          &copy; <span id="year">2025</span> ainfo.dev. All rights reserved.
        </p>
      </div>
    </footer>

    <script src="script.js"></script>
  </body>
</html>
